import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.HashPartitioner
import org.apache.spark.streaming.kafka._

import com.mongodb.casbah.Imports.MongoDBObject
import com.mongodb.casbah.Imports.MongoClient
import com.mongodb.casbah.Imports.DBObject

import scala.concurrent.{Await, Future}
import scala.concurrent.ExecutionContext.Implicits.global
import scala.concurrent.duration._

import scala.util.control.Breaks._

import akka.actor._

object Streaming{	
	def main(args: Array[String]){
		val conf = new SparkConf().setMaster("spark://192.168.0.241:7077").setAppName("myApp")
		val sc = new SparkContext(conf)
		var isFirst = 1 // if 1, first
		var flag = 1
		
		var printON: Boolean = false
		var printTime: Boolean = false
		var streamCount = 1
		
		var cachedPRDD: org.apache.spark.rdd.RDD[(Int, String)] = null
		var missPRDD: org.apache.spark.rdd.RDD[(Int, String)] = null
		var LRUKey: org.apache.spark.rdd.RDD[(Int, Int)] = null
		var LRUKey2: org.apache.spark.rdd.RDD[(Int, Int)] = null
		//var LRUKey: Array[String] = null
		var joinedPRDD_hit: org.apache.spark.rdd.RDD[(Int, (String, String))] = null
		var joinedPRDD_miss: org.apache.spark.rdd.RDD[(Int, (String, String))] = null
		
		val ssc = new StreamingContext(sc, Seconds(3))
		val stream = ssc.socketTextStream("192.168.0.243", 9999)

		
		val mongoClient = MongoClient("192.168.0.242", 27020)
		val db = mongoClient("s1000")
		val coll = db("part")
		
		var streaming_data_all: Int = 0
		var time_all = 0
		
		/*
		var num = 0
		var tf = true
		while(tf){
			if(num == 1000){
				tf = false
			}
			num = num + 1
		}*/
		
		var hashP = new HashPartitioner(16)
		println("hi!!!!")
		
		stream.foreachRDD({ rdd =>
			val tStart = System.currentTimeMillis
			var sRdd = rdd.repartition(16)
			println(sRdd.count)
			
			println("============== Stream num [" + streamCount + "] =================")
			
			var t0 = System.currentTimeMillis
			
			/* Discretize the input stream into RDDs */
			// input RDD
			// tbl
			//var inputPRDD = rdd.map{ s=>  var tmp = s.split("\""); (tmp.lift(3).get.toInt, s) }
			var inputPRDD = sRdd.map{ s=>  var tmp = s.split("\""); (tmp.lift(3).get.toInt, s) }.partitionBy(hashP)
			//var inputPRDD = rdd.map{ s=>  var tmp = s.split("\""); (tmp.lift(3).get.toInt, s) }.partitionBy(new HashPartitioner(16)).persist
			inputPRDD.persist
			inputPRDD.count
			
			var inputCount = inputPRDD.count
			var missCount: Long = 0
			var missKeyCount: Long = 0
			
			println(">> input data: " + inputCount)
			//streaming_data_all = streaming_data_all + inputCount.toInt
			
			var t1 = System.currentTimeMillis
			if(printTime)	println("[TIME] create inputPRDD: " + (t1 - t0) + " ms")
			//if(true)	println("[TIME] create inputPRDD: " + (t1 - t0) + " ms")
			/* ------------------------------------------- */
						
			/* hit data join */
			val b = new Thread(){
				override def run = {
					if(isFirst == 0){
						t0 = System.currentTimeMillis
						joinedPRDD_hit = inputPRDD.join(cachedPRDD, hashP)
						//if(printON)	println(">> joinedPRDD_hit count: " + joinedPRDD_hit.count)
						//else joinedPRDD_hit.count
						t1 = System.currentTimeMillis
						if(printTime)	println("[TIME] join - hit data: " + (t1 - t0) + " ms")
						if(printTime) println("[Thread_1 : TIME] " + (t1 - t0) + " ms")
						//joinedPRDD_hit.persist
						//joinedPRDD_hit.count
					}
				}
			}.start()
			/* --------------------------------------------------------------------------------- */
			
			val f = Future{ // for missed data
				/* subtractByKey */
				var missKey: org.apache.spark.rdd.RDD[Int] = null
				var cachedCount: Int = 0
				
				var t0 = System.currentTimeMillis				
				if(isFirst == 1){
					//cachedCount = 0
					missPRDD = inputPRDD
				}else{
					//cachedCount = cachedPRDD.count.toInt
					missPRDD = inputPRDD.subtractByKey(cachedPRDD)
					//missPRDD = inputPRDD.subtractByKey(cachedPRDD, hashP)
				}
				
				missCount = missPRDD.count
				missCount				
				//if(printON)	println(">> cached data: " + cachedCount)
				//else cachedCount
				//if(true)	println(">> miss data: " + missCount)
				//else missCount
				
				var t1 = System.currentTimeMillis	
				//if(true)println("[TIME] create missPRDD: " + (t1 - t0) + " ms")
				if(printTime)println("[TIME] create missPRDD: " + (t1 - t0) + " ms")
				
				/* --------------------------------------------------------------------------------- */
				
				if(streamCount == 2){
					Thread sleep(1000000)
				}
				
				t0 = System.currentTimeMillis
				missKey = missPRDD.keys.distinct
				missKeyCount = missKey.count
				
				var itemCount = missKeyCount / 4
				
				//if(printON)	println(">> create miss keys: " + missKeyCount)
				//else missKeyCount
				if(printON)	println(">> make query")

				/* generate queries using missedRDD */
				var qList: List[com.mongodb.casbah.commons.Imports.DBObject] = null
				var qList2: List[com.mongodb.casbah.commons.Imports.DBObject] = null
				var qList3: List[com.mongodb.casbah.commons.Imports.DBObject] = null
				var qList4: List[com.mongodb.casbah.commons.Imports.DBObject] = null
			
				var count = 0

				missKey.collect.foreach{ s => var tmp = MongoDBObject("partkey" -> s); 
					if(count == 0){ qList = List(tmp) }
					else if(count == itemCount){qList2 = List(tmp)}
					else if(count == itemCount*2){qList3 = List(tmp)}
					else if(count == itemCount*3){qList4 = List(tmp)}
					else if(count < itemCount){qList = qList:+tmp}
					else if(count > itemCount && count < itemCount*2){qList2 = qList2:+tmp}
					else if(count > itemCount*2 && count < itemCount*3){qList3 = qList3:+tmp}
					else{ qList4 = qList4 :+ tmp }; 
				count = count + 1 }
				
				var q = MongoDBObject("$or" -> qList)
				var q2 = MongoDBObject("$or" -> qList2)
				var q3 = MongoDBObject("$or" -> qList3)
				var q4 = MongoDBObject("$or" -> qList4)
				
				t1 = System.currentTimeMillis
				if(printTime)println("[TIME] create query: " + (t1 - t0) + " ms")
				
				//t0 = System.currentTimeMillis
				var documents = coll.find(q)
				var documents2 = coll.find(q2)
				var documents3 = coll.find(q3)
				var documents4 = coll.find(q4)
				//t1 = System.currentTimeMillis
				//println("[TIME] get data from mongodb: " + (t1 - t0) + " ms")
				/* --------------------------------------------------------------------------------- */
				
				/* execute generated queries */
				var tmp = Array("a")
				var tmp2 = Array("a")
				var tmp3 = Array("a")
				var tmp4 = Array("a")
				
				val queryThread1 = new Thread(){
					override def run = {
						documents.foreach(s => tmp = tmp:+ s.toString)
					}
				}
			
				val queryThread2 = new Thread(){
					override def run = {
						t0 = System.currentTimeMillis
						documents2.foreach(s => tmp2 = tmp2:+ s.toString)
					}
				}				
				
				val queryThread3 = new Thread(){
					override def run = {
						documents3.foreach(s => tmp3 = tmp3:+ s.toString)
					}
				}
				
				queryThread1.start
				queryThread2.start
				queryThread3.start

				documents4.foreach(s => tmp4 = tmp4:+ s.toString)
				var done = false
				
				queryThread1.join()
				queryThread2.join()
				queryThread3.join()
				
				var tmpAll = tmp.drop(1).union(tmp2.drop(1)).union(tmp3.drop(1)).union(tmp4.drop(1))
				
				t1 = System.currentTimeMillis
				if(printTime)println("[TIME] get data from mongodb using cursor: " + (t1 - t0) + " ms")
				/* --------------------------------------------------------------------------------- */
				
				
				/* transform query results to RDD */
				t0 = System.currentTimeMillis
				var newRDD = sc.parallelize(tmpAll)
				var newPRDD = newRDD.map{ s=> {var tmpAll=s.split(" "); (tmpAll.lift(10).get.toInt, s) } }
				//var newPRDD = newRDD.map{ s=> {var tmpAll=s.split(" "); (tmpAll.lift(10).get.toInt, s) } }.partitionBy(new HashPartitioner(16))
				//newPRDD.persist
				
				var newCount = newPRDD.count
				
				if(printON)	println(">> newPRDD count: " + newCount)
				//else newCount
				
				t1 = System.currentTimeMillis
				if(printTime)	println("[TIME] create newPRDD: " + (t1 - t0) + " ms")
				/* --------------------------------------------------------------------------------- */				
				
				
				/* cache management */
				val cacheThread = new Thread(){
					override def run = {
						t0 = System.currentTimeMillis
						/* union */
						if(isFirst == 1){
							cachedPRDD = newPRDD.partitionBy(hashP)
							//cachedPRDD = newPRDD.repartition(16)
							//cachedPRDD = newPRDD
						}else{
							cachedPRDD = cachedPRDD.union(newPRDD).partitionBy(hashP)
							//cachedPRDD = cachedPRDD.union(newPRDD).partitionBy(new HashPartitioner(16))
							//cachedPRDD = cachedPRDD.union(newPRDD).repartition(16)
						}
						cachedPRDD.persist
						//cachedPRDD.count
						//cachedCount = cachedPRDD.persist.count
						t1 = System.currentTimeMillis
						if(printON)	println(">> cachedPRDD count: " + cachedPRDD.count)
						if(printTime)	println("[TIME] create cachedPRDD: " + (t1 - t0) + " ms")
						//else cachedPRDD.count
						/* --------------------------------------------------------------------------------- */
						
						/* reorganization
						t0 = System.currentTimeMillis
						if(isFirst){						
							LRUKey = missKey.map(s => (s, streamCount))
							
							//LRUKey.take(10).foreach(println)
						}else{
							//LRUKey.take(10).foreach(println)
							
							LRUKey = LRUKey.subtractByKey(inputPRDD)
							//LRUKey = LRUKey.sortBy(s => s._2)
							
							LRUKey = LRUKey.union(inputPRDD.keys.distinct.map(s => (s,streamCount)))
							LRUKey.repartition(16)
						}
						LRUKey.persist
						
						
						//LRUKey.take(10).foreach(println)
						
						if(printON)	println(">> LRUKey count: " + LRUKey.count)
						else LRUKey.count
						t1 = System.currentTimeMillis
						if(printTime)println("[TIME] create LRUKeyPRDD: " + (t1 - t0) + " ms")
						*/
						
						/*//if(sumCache > 10000){
						if(LRUKey.count > 10000){
							println(">> caching change")
							//var subCount = LRUKey.count.toInt - 10000
							LRUKey = LRUKey.sortBy(s => s._2)
							var rmCount = newPRDD.count.toInt
							var rmData = sc.parallelize(LRUKey.take(rmCount))

						
							cachedPRDD = cachedPRDD.subtractByKey(rmData)
							//LRUKey = sc.parallelize(LRUKey.collect.drop(subCount))
						
						}*/
						/* --------------------------------------------------------------------------------- */
					}
				}
				cacheThread.start
				
				/* join missed data */
				//if(isFirst == 0){
					t0 = System.currentTimeMillis
					joinedPRDD_miss = missPRDD.join(newPRDD)
					if(printON)	println(">> joinedPRDD_miss count: " + joinedPRDD_miss.count)
					//else joinedPRDD_miss.count
					t1 = System.currentTimeMillis
					if(printTime)	println("[TIME] join - miss data: " + (t1 - t0) + " ms")
					
					//joinedPRDD_miss.persist
					//joinedPRDD_miss.count
				//}
				cacheThread.join()
				/* --------------------------------------------------------------------------------- */
			} // Future
						


			// start Future f (missed data join)		
			t0 = System.currentTimeMillis
			val n =Await.result(f, scala.concurrent.duration.Duration.Inf)
			t1 = System.currentTimeMillis
			if(printTime) println("[Thread_2 : TIME] " + (t1 - t0) + " ms")
			//b.join()
			
			
			var outputCount: Long = 0
			
			/* union (joinedRDD_hit, joinedRDD_missed) */
			t0 = System.currentTimeMillis
			if(isFirst == 0){
				outputCount = joinedPRDD_hit.union(joinedPRDD_miss).count
				if(printON)	println(">> outputPRDD count: " + outputCount)
				//else joinedPRDD_hit.union(joinedPRDD_miss).count
			}else{
				outputCount = joinedPRDD_miss.count
				//outputCount = inputPRDD.join(cachedPRDD).count
				if(printON)	println(">> outputPRDD count: " + outputCount)
			}
			t1 = System.currentTimeMillis
			if(printTime) println("[TIME] union output data: " + (t1 - t0) + " ms")
			/* --------------------------------------------------------------------------------- */
			
			
			
			streamCount = streamCount + 1
			
			println(">> output data: " + outputCount)
			
			val tEnd = System.currentTimeMillis
			println("[Time] stream: " + (tEnd - tStart) + " ms")
			
			streaming_data_all = streaming_data_all + outputCount.toInt
			println(">> streaming data all: " + streaming_data_all)
			
			var timetmp: Int = tEnd.toInt - tStart.toInt
			time_all = time_all + timetmp
			
			println("[Time] total: " + time_all + " ms")
			
			/*
			if(isFirst == 0){
				val asd = new Thread(){
					override def run = {
						Thread sleep(100000)
						Thread sleep(100000)
						Thread sleep(100000)
					}
				}
				asd.start
				asd.join()
			}*/
		
			inputPRDD.unpersist()
			//if(isFirst == 0){
				//joinedPRDD_hit.unpersist()
				//joinedPRDD_miss.unpersist()
			//}
			isFirst = 0
			
			
		})
		
		ssc.start()
		ssc.awaitTermination()
	}
}






/*try{
	missPRDD = inputPRDD.subtractByKey(cachedPRDD)
	println(">> cached data: " + cachedCount)
	LRUKey = LRUKey.subtract(missKey).union(missKey)
} catch {
	case e: NullPointerException => {
		missPRDD = inputPRDD
		println(">> There is no cached data")
		LRUKey = missKey
	}
}

missCount = missPRDD.count
println(">> miss data: " + missCount)

missKey = missPRDD.keys.distinct
*/
