import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.HashPartitioner

import com.mongodb.casbah.Imports.MongoDBObject
import com.mongodb.casbah.Imports.MongoClient
import com.mongodb.casbah.Imports.DBObject

import scala.concurrent.{Await, Future}
import scala.concurrent.ExecutionContext.Implicits.global
import scala.concurrent.duration._

import scala.util.control.Breaks._

import akka.actor._

object Streaming{	
	def main(args: Array[String]){
		val conf = new SparkConf().setMaster("spark://192.168.0.241:7077").setAppName("myApp")
		val sc = new SparkContext(conf)
		var isFirst = 1 // if 1, first
		var flag = 1
		
		var printON: Boolean = false
		var printTime: Boolean = false
		var streamCount = 1
		
		var cachedPRDD: org.apache.spark.rdd.RDD[(Int, String)] = null
		var missPRDD: org.apache.spark.rdd.RDD[(Int, String)] = null
		var LRUKey: org.apache.spark.rdd.RDD[(Int, Int)] = null
		var LRUKey2: org.apache.spark.rdd.RDD[(Int, Int)] = null
		//var LRUKey: Array[String] = null
		var joinedPRDD_hit: org.apache.spark.rdd.RDD[(Int, (String, String))] = null
		var joinedPRDD_miss: org.apache.spark.rdd.RDD[(Int, (String, String))] = null
		
		val ssc = new StreamingContext(sc, Seconds(3))
		val stream = ssc.socketTextStream("192.168.0.243", 9999)
		
		val mongoClient = MongoClient("192.168.0.242", 27020)
		val db = mongoClient("s1000")
		val coll = db("part")
		
		var streaming_data_all: Int = 0
		var time_all = 0
		var inputRDD = sc.textFile("/streaming_data/lineitem_3.json")
		println("input: " + inputRDD.count)
		
		
		var t = System.currentTimeMillis
		
		var inputRDD1 = sc.textFile("/streaming_data/lineitem_1.json")
		var inputPRDD1 = inputRDD1.map{ s=>  var tmp = s.split("\""); (tmp.lift(3).get.toInt, s) }
		var missKey = inputPRDD1.keys.distinct
		
		var qList: List[com.mongodb.casbah.commons.Imports.DBObject] = null
		var count = 0

		missKey.collect.foreach{ s => var tmp = MongoDBObject("partkey" -> s); if(count == 0){ qList = List(tmp) }else{ qList = qList :+ tmp }; count = count + 1 }

		var q = MongoDBObject("$or" -> qList)
		var documents = coll.find(q)
		var tmp = Array("a")
		documents.foreach(s => tmp = tmp:+ s.toString)
		tmp = tmp.drop(1)
		
		var mongoRDD1 = sc.parallelize(tmp)
		var mongoPRDD1 = mongoRDD1.map{ s=> {var tmp=s.split(" "); (tmp.lift(10).get.toInt, s) } }.partitionBy(new HashPartitioner(16))
		
		var joinedPRDD1: org.apache.spark.rdd.RDD[(Int, (String, String))]= null
		//var a = new Thread() {
			//override def run = {
				var t0 = System.currentTimeMillis
				joinedPRDD1 = inputPRDD1.join(mongoPRDD1)
				joinedPRDD1.persist
				
				println("joinedPRDD1: " + joinedPRDD1.count)

				var t1 = System.currentTimeMillis
				println("[TIME]: " + (t1 - t0) + " ms")
			//}
		//}
		//a.start
		
/////////////////////////////////////////////////////////////////////////		
		

		var inputRDD2 = sc.textFile("/streaming_data/lineitem_2.json")
		var inputPRDD2 = inputRDD2.map{ s=>  var tmp = s.split("\""); (tmp.lift(3).get.toInt, s) }
		var missKey2 = inputPRDD2.keys.distinct
		
		var qList2: List[com.mongodb.casbah.commons.Imports.DBObject] = null
		var count2 = 0

		missKey2.collect.foreach{ s => var tmp = MongoDBObject("partkey" -> s); if(count2 == 0){ qList2 = List(tmp) }else{ qList2 = qList2 :+ tmp }; count2 = count2 + 1 }

		var q2 = MongoDBObject("$or" -> qList2)
		var documents2 = coll.find(q2)
		var tmp2 = Array("a")
		documents2.foreach(s => tmp2 = tmp2:+ s.toString)
		tmp2 = tmp2.drop(1)
		
		var mongoRDD2 = sc.parallelize(tmp2)
		var mongoPRDD2 = mongoRDD2.map{ s=> {var tmp=s.split(" "); (tmp.lift(10).get.toInt, s) } }.partitionBy(new HashPartitioner(16))
	
		
		var t2 = System.currentTimeMillis
		var joinedPRDD2 = inputPRDD2.join(mongoPRDD2)
		//println("joinedPRDD2: " + joinedPRDD2.count)
		
		var t3 = System.currentTimeMillis
		println("[TIME]: " + (t3 - t2) + " ms")
		
		//a.join()
		var unionPRDD = joinedPRDD1.union(joinedPRDD2)
		println("union: " + unionPRDD.count)
		var t5 = System.currentTimeMillis
		println("[TIME total]: " + (t5 - t) + " ms")
		
		
		var hashP = new HashPartitioner(16)
	}
}






/*try{
	missPRDD = inputPRDD.subtractByKey(cachedPRDD)
	println(">> cached data: " + cachedCount)
	LRUKey = LRUKey.subtract(missKey).union(missKey)
} catch {
	case e: NullPointerException => {
		missPRDD = inputPRDD
		println(">> There is no cached data")
		LRUKey = missKey
	}
}

missCount = missPRDD.count
println(">> miss data: " + missCount)

missKey = missPRDD.keys.distinct
*/

